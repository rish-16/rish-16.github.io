---
title: 'Elo: From Chess Ratings to Loss Functions'
date: 2022-01-24
permalink: /posts/2022/01/elo-loss/
tags:
    - loss functions
    - optimistation
    - experiments
---

## Preface

I rewatched the DeepMind documentary on AlphaGo recently on whim. They mentioned the [Elo Rating](https://en.wikipedia.org/wiki/Elo_rating_system) used to rate players in competitive games like Chess, Go, Baseball, general boardgames, and more. This suddenly got me curious about the scoring system and I read up quite a bit about it.

In this post, I document my rather crazy idea of repurposing the Elo Rating formula into a loss function to train networks and somehow, it worked out pretty well. I describe my experimental setup and showcase some interesting results.

## Elo Rating
Elo rating is used to calculate relative skills levels between players. Players in such games all start off with a base rating and move up the ladder based on calculations involving the Elo rating. It looks rather convoluted and a quick internet search doesn't give much information on it either. Let me quickly break it down for you here. For starters, given two players **A** and **B**, the formula looks like this:

PICTURE OF RAW ELO FORMULA

$$E_A$$ denotes the expected probability of **A** winning the game giving $$R_A$$ and $$R_B$$, the ratings of players **A** and **B** respectively; ultimately, we're calculating $$P(A wins)$$. You can do this for player **B** too: in the denominator, you have to switch it to $$R_A - R_B$$ instead. This function always gives a probability value $$\leq 1$$. 

To update the scores of the two players, the following formula is used:

UPDATE FORMULA

Of course, this update step is not of concern in this blog post. It's just a funky detail I decided to add for brevity.

## Using Elo as a loss function

The Elo score function $$E_A$$ for some player **A** is an example of a logistic function that has the following general form:

LOGISTIC FUNCTION

in fact, when $$L=1$$, $$k=1$$, and $$x_0 = 1$$, it's called a _Sigmoid_ i.e., the activation function used to inject non-linearity into perceptrons and neural networks; it's mainly used in binary classification problems given some classification threshold. Here, however, I use this Elo logistic function as a loss function, not an activation. It's best to make that distinction before showcasing some interesting results.

This so-called **Elo Loss** can be formulated as follows:

ELO LOSS

where $$\hat{y}$$ is the prediction, $$y$$ is the label, and $$m$$ is the batch/minibatch size.

## Results and Discussion
I trained a standard MLP using Elo Loss and standard Cross Entropy Loss on MNIST and CIFAR-10. I repeated the experiments with a CNN using a similar set-up but with the additional Fashion MNIST dataset included. I've embedded the _Weights & Biases_ plots for your reference:

The mo